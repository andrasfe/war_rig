# War Rig Configuration
# Complete reference for all environment variables

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Provider name - determines which LLM backend to use
# Built-in: "openrouter"
# Custom: Any provider registered via entry points (see war_rig/providers/README.md)
LLM_PROVIDER=openrouter

# Default model for all agents (can be overridden per-agent below)
# Used when agent-specific model is not set
LLM_DEFAULT_MODEL=anthropic/claude-sonnet-4-20250514

# Request timeout in seconds (for slow models or large prompts)
# Default is 600s (10 min) if not set
LLM_REQUEST_TIMEOUT=600

# =============================================================================
# OPENROUTER SETTINGS (when LLM_PROVIDER=openrouter)
# =============================================================================

OPENROUTER_API_KEY=sk-or-v1-your-key-here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Optional: For OpenRouter rankings/analytics
OPENROUTER_SITE_URL=https://github.com/your-org/war-rig
OPENROUTER_SITE_NAME=War Rig Mainframe Docs

# =============================================================================
# ANTHROPIC SETTINGS (when LLM_PROVIDER=anthropic)
# =============================================================================

# ANTHROPIC_API_KEY=sk-ant-your-key-here

# =============================================================================
# WAR RIG IDENTIFICATION
# =============================================================================

# Unique identifier for this War Rig instance
RIG_ID=ALPHA

# =============================================================================
# AGENT MODEL ASSIGNMENTS
# =============================================================================

# Scribe (Documenter) - Needs strong code comprehension and detail orientation
# Good at: filling templates, extracting information, following instructions
SCRIBE_MODEL=anthropic/claude-sonnet-4-20250514
SCRIBE_TEMPERATURE=0.3
SCRIBE_MAX_PROMPT_TOKENS=15000
SCRIBE_MAX_COMPLETION_TOKENS=8192

# Challenger (Validator) - Needs critical thinking, questioning ability
# Good at: finding gaps, asking probing questions, verification
CHALLENGER_MODEL=openai/gpt-4o-2024-11-20
CHALLENGER_TEMPERATURE=0.5
CHALLENGER_MAX_PROMPT_TOKENS=15000
CHALLENGER_MAX_COMPLETION_TOKENS=4096

# Imperator (Reviewer) - Needs judgment, decision making
# Good at: evaluation, synthesis, final decisions
IMPERATOR_MODEL=anthropic/claude-sonnet-4-20250514
IMPERATOR_TEMPERATURE=0.2
IMPERATOR_MAX_PROMPT_TOKENS=15000
IMPERATOR_MAX_COMPLETION_TOKENS=8192

# Fallback model when primary Imperator model fails (stream stall, etc.)
# If not set, no fallback is attempted and the error propagates normally
# IMPERATOR_FALLBACK_MODEL=google/gemini-2.0-flash-001

# =============================================================================
# SUPER-SCRIBE (Automatic rescue for blocked tickets)
# =============================================================================

# When tickets fail on all normal workers, Super-Scribe automatically retries
# with a stronger model. Uses Opus by default for maximum capability.
SUPER_SCRIBE_MODEL=anthropic/claude-opus-4-20250514
SUPER_SCRIBE_MAX_PROMPT_TOKENS=30000
NUM_SUPER_SCRIBES=1

# =============================================================================
# MINION SCRIBES (Fast models for call semantics analysis)
# =============================================================================

# Enable/disable call semantics analysis (data flow between paragraphs)
ENABLE_CALL_SEMANTICS=true

# Fast, cheap model for parallel analysis
MINION_SCRIBE_MODEL=anthropic/claude-3-haiku-20240307
NUM_MINION_SCRIBES=4
MINION_SCRIBE_BATCH_SIZE=5

# =============================================================================
# CITADEL-GUIDED PROCESSING
# =============================================================================

# Files exceeding these thresholds use batched per-paragraph processing
CITADEL_GUIDED_THRESHOLD_LINES=1000
CITADEL_GUIDED_THRESHOLD_PARAGRAPHS=10

# Maximum paragraphs per batch in Citadel-guided processing
CITADEL_MAX_PARAGRAPHS_PER_BATCH=10

# =============================================================================
# WORKFLOW SETTINGS
# =============================================================================

# Number of redundant teams processing in parallel
NUM_TEAMS=1

# Maximum iterations before forced approval
MAX_ITERATIONS=3

# Maximum questions Challenger can ask per round
MAX_QUESTIONS_PER_ROUND=5

# Challenger paragraph sampling (reduces context)
CHALLENGER_PARAGRAPH_SAMPLE_SIZE=5

# Maximum Chrome tickets Imperator can issue per round
MAX_CHROME_TICKETS=5

# =============================================================================
# WORKER POOL SETTINGS
# =============================================================================

# Number of parallel workers
NUM_SCRIBES=3
NUM_CHALLENGERS=2

# Max Program Manager cycles before forced completion
PM_MAX_CYCLES=5

# Agent cycle limits
MAX_CHALLENGER_CYCLES=2
MAX_IMPERATOR_CYCLES=1

# =============================================================================
# ERROR HANDLING & RECOVERY
# =============================================================================

# Exit immediately on first error
EXIT_ON_ERROR=true

# Max retry attempts per ticket before fatal exit
MAX_TICKET_RETRIES=5

# Formatting error recovery (for recoverable LLM output errors)
ENABLE_FORMATTING_RECOVERY=true
MAX_FORMATTING_FIX_ATTEMPTS=2
FORMATTING_FIX_TIMEOUT=120

# Use compact holistic review (Tier 1) for reduced token usage
USE_COMPACT_HOLISTIC_REVIEW=true

# =============================================================================
# INPUT/OUTPUT PATHS
# =============================================================================

INPUT_DIRECTORY=./input
OUTPUT_DIRECTORY=./output

# =============================================================================
# PREPROCESSING
# =============================================================================

EXTRACT_PARAGRAPHS=true
EXTRACT_PERFORMS=true
EXTRACT_CALLS=true
EXTRACT_COPYBOOKS=true
EXTRACT_SQL=true
EXTRACT_CICS=true

# =============================================================================
# LOGGING
# =============================================================================

LOG_LEVEL=INFO
LOG_FILE=./output/war_rig.log
SAVE_DIALOGUES=true

# Error logging file (detailed error reports)
# ERR_FILE=./output/errors.jsonl

# =============================================================================
# CHECKPOINTING
# =============================================================================

CHECKPOINT_ENABLED=true
CHECKPOINT_FREQUENCY=per_program
CHECKPOINT_DIRECTORY=./output/checkpoints

# =============================================================================
# BEADS INTEGRATION (Issue Tracking)
# =============================================================================

# Enable beads ticket creation for agent issues
BEADS_ENABLED=false
BEADS_DRY_RUN=false

# =============================================================================
# ALTERNATIVE MODEL OPTIONS (examples)
# =============================================================================

# Budget-friendly:
# SCRIBE_MODEL=meta-llama/llama-3.3-70b-instruct
# CHALLENGER_MODEL=google/gemini-2.0-flash-001
# IMPERATOR_MODEL=meta-llama/llama-3.3-70b-instruct

# Premium:
# SCRIBE_MODEL=anthropic/claude-opus-4-20250514
# CHALLENGER_MODEL=openai/gpt-4o
# IMPERATOR_MODEL=anthropic/claude-opus-4-20250514

# =============================================================================
# QUESTION RESOLUTION (Automatic via CodeWhisper)
# =============================================================================

# Enable automatic open question resolution after each cycle
QUESTION_RESOLUTION_ENABLED=false

# Max questions to attempt per cycle (prevents runaway API costs)
QUESTION_RESOLUTION_MAX_PER_CYCLE=10

# Timeout per question in seconds
QUESTION_RESOLUTION_TIMEOUT=120

# Minimum confidence to accept: HIGH, MEDIUM, LOW
QUESTION_RESOLUTION_MIN_CONFIDENCE=MEDIUM

# CodeWhisper SDK settings for resolution
QUESTION_RESOLUTION_CW_MAX_ITERATIONS=8
QUESTION_RESOLUTION_CW_TEMPERATURE=0.2

# Also resolve inline README questions
QUESTION_RESOLUTION_RESOLVE_README=true

# =============================================================================
# CODEWHISPER SETTINGS (Interactive CLI)
# =============================================================================

# CodeWhisper uses LLM_PROVIDER and IMPERATOR_MODEL by default
# Override with CODEWHISPER_ prefix:
# CODEWHISPER_MODEL=google/gemini-2.0-flash-001
# CODEWHISPER_TEMPERATURE=0.3
# CODEWHISPER_MAX_TOKENS=4096

# Minion context threshold for summarization (chars)
# MINION_CONTEXT_THRESHOLD=32000

# =============================================================================
# KNOWLEDGE GRAPH (Structural relationship tracking)
# =============================================================================

# Enable knowledge graph construction during documentation passes
KNOWLEDGE_GRAPH_ENABLED=true

# Path to the SQLite database for the graph store
KNOWLEDGE_GRAPH_DB_PATH=./output/knowledge_graph.db

# Maximum tokens for graph context injection into Scribe/Challenger prompts
KNOWLEDGE_GRAPH_MAX_CONTEXT_TOKENS=500

# Number of hops for neighborhood queries (1 = direct, 2 = includes JCL context)
KNOWLEDGE_GRAPH_NEIGHBORHOOD_HOPS=2

# Triple delta threshold for convergence detection (0.05 = 5%)
KNOWLEDGE_GRAPH_CONVERGENCE_THRESHOLD=0.05

# Extract ground-truth triples from preprocessor output (COBOL, JCL)
KNOWLEDGE_GRAPH_EXTRACT_FROM_PREPROCESSORS=true

# Instruct Scribe to emit structured triples alongside documentation
KNOWLEDGE_GRAPH_EMIT_TRIPLES_FROM_SCRIBE=true

# Enable Challenger structural cross-check against graph data
KNOWLEDGE_GRAPH_CHALLENGER_CROSS_CHECK=true

# =============================================================================
# END OF CONFIGURATION
# =============================================================================
